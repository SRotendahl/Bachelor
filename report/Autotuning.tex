\section{Autotuning}
The process of autotuning a Futhark program is, to automatically pick the fastest combination (or single) code versions, based on the hardware the program is executed on, and some representative data. In the process of selecting the appropriate code version we have three important terms we, that we be central to this report, and therefore we wish to clearly define them in the context of this report;
\begin{basedescript}{
		\desclabelstyle{\multilinelabel}
		\desclabelwidth{2cm}}
	\item [Threshold parameter] or just \textit{threshold} is a value that symbolizes the capacity for parallelism (memory, thread count etc.), of the hardware the program is executed on. 
	\item [Dataset value] is a value that represent some of the dataset given. For example for the outer \texttt{map} in matrix-matrix multiplication, the dataset value would (primarily) be constructed by the number of rows.
	\item [Predicate] threshold comparison, or just comparison, is a comparison between a threshold and a dataset value that guard which code version to use.   
\end{basedescript}
To pick the best version, we need to tune the threshold parameters, to values that will result en the combination of code versions that gives the fastest runtime. This process is called tuning. The guarding predicates are of the form $threshod \; \leq \; dataset\; value$. By default the threshold parameter is set to a value off $2^{15}$, as an estimate, but is likely sub-optimal. With these terms, we can now fill out the example in Figure \ref{MatMultTree}, getting Figure \ref{MatMultTreeFilled}. 
\begin{center}
	\centering 
	\input{MatMultTreeFilled}
	\captionof{figure}{The tree generated by matrix-matrix multiplication that is to be tuned. ($t_i$) is a threshold, ($dVal_i$) is a dataset value, and (T) and (F) are indicative of the path we take, based on the threshold comparison.}
	\label{MatMultTreeFilled}
\end{center}
To inspect the structure of these predicates and thresholds parameters further, lets look at a more complex Futhark program, from Futhark bench, called \textit{LocVolCalib};
\begin{center}
	\centering
	\input{LocValCalibTree}
	\captionof{figure}{The dependencies between thresholds, of the test program \texttt{LocVolCalib.fut}. (Th) is a threshold comparison, (V) is a code version, and (T) and (F) are indicative of the path we take, based on the threshold comparison}
	\label{LocVolCalibTree}
\end{center}
\noindent To tune a program we need to examine each execution path through the tree. It is important not to get an end node (code version) confused with a version of the program. Two example of paths through the tree in Figure \ref{LocVolCalibTree}, that shows this, could be;
\begin{itemize}
	\item \texttt{\{(Th4, False), (Th5, False), (Th6, False), (Th7, True)\}}
	\item \texttt{\{(Th4, False), (Th5, False), (Th6, False), (Th7, False), (Th8, False), (Th9, True), (Th16, False), (Th17, True)\}}
\end{itemize}  
The first path is simple, the code represented by \texttt{T4, T5, T6} is executed in parallel, where everything after it, is executed sequentially. The second path is more interesting, \texttt{T7} has two child nodes, that are reached with a false comparison. Here it is clear that two end nodes are reached, namely \texttt{(V6, V9)}, and these two code versions are then combined into one program. This is also important to note, because we could have a forest, instead of a single tree, and this would leave multiple code versions, that are to be combined.

\subsection{Search space and portability}
%13
There are two main reasons for the use of autotuning (compared to manual tuning), the search space of different threshold settings, and the portability of the code.
\paragraph{The search space} for thresholds is quite large. The threshold parameters has a value of $2^{15}$ as a default, lets us then consider matrix-matrix multiplication again (Figure \ref{MatMultTreeFilled}). Here we have 4 thresholds, which would leave $(2^{15})^4 \approx 1.153x10^{18}$ different threshold parameter settings (and this is assuming that $2^{15}$ is an appropriate maximum value for all hardware, which is not the case). However we see that a setting can only end in five different executions. This is due to a data value of $128$ would result in the same in all the settings from $2^{15}$ down $128$. So to manually tune the search space is incredible tedious and repetitive.
\paragraph{Portability} of code is important, especially for GPU's due to the low level link between the code and the architecture (GPU threads, blocks/groups etc.). When tuning we tune to specific hardware, and representative datasets, so as soon as the code is to be executed on a different system (with a potentially different size of input data), the tuning performed would be useless (and in the worst case even detrimental to performance).\\

To illustrate the problem of repetitiveness through the search space we see Figure \ref{MatMultTreeFilled}, with the thresholds $t = [10, 20, 30, 40]$, and the dataset values $dVal = [20, 30, 40, 50]$, so the predicate of $t_0 \leq dVal_0$ would result in taking the \texttt{true} branch, however this is also the case for $t_0 = 11$, making these two different settings, having the same dynamic behavior.