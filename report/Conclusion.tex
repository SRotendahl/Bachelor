\section{Conclusion}
\label{conclusion}
Our autotuner works as intended, it finds all the possible combinations of paths for a program, with some amount of datasets, returning the best one for all datasets. The autotuner achieves this with an with an exhaustive search through all these combinations. An exhaustive search guarantees that we will always find the optimal path, which was not the case with the existing autotuner, therefore making our autotuner more reliable.

Being that Futhark is an ongoing research project, changes to it is to be
expected. We have taken this into account, and braced the autotuner for these
changes. While a Futhark programs thresholds parameters cannot currently be
structured as  a forest it might do in the future. For this reason we have
implemented the autotuner such that it works on forests. While our
implementation will not support nodes in the program tree having multiple
parents but a method to support this is explained.

With an exhaustive search comes an exponential amount of paths in the worst case, therefore our autotuner gets exponentially slower when tuning for multiple dataset. We saw this in the benchmark for \texttt{LocVolCalib} as seen in Figure \ref{LocVolCalib-SmallMediumLarge}. For that program it took about 23 hours to autotune with all three datasets due to the number of paths being 5463. 

We have put forth an idea for future work that can be done on the autotuner,
and a optimization to the current implementation. If using a single synthetic datasets,
as suggested in section \ref{synData}, to represent the workload of
a program in a single dataset is possible the \texttt{LocVolCalib} program will
result in 13 paths and therefore a much faster tuning, which could be even
faster with the optimization of having slower executions time out during
autotuning.

While the GPU tested on is not state-of-the-art, and \texttt{LocVolCalib} might
be considered large for a Futhark program, we will conclude that the notion of
Futhark programs being small enough to comfortably autotune exhaustively,
should be reconsidered, unless it is possible to create synthetic datasets that
minimizes the amount of datasets needed.
