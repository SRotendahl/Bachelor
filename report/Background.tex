\section{Background}
A common way to increase computer performance, is to increase the capacity for parallelism. For practical usage, however, this is difficult to implement, due to low-level GPU-specific languages requiring domain specific knowledge to make full use of that capacity. A wast amount of work has gone into transforming high-level hardware-agnostic code into these low-level GPU-specific languages \cite{inc-flat}. 

\subsection{Futhark}
The programming language \textbf{Futhark} aims to solve this problem. The creator of Futhark writes the purpose, of the language, nicely on the home page for the language \textit{"Because it’s nicer than writing CUDA or OpenCL by hand!"} \cite{futhark-home}. On the same page, Futhark is described, more precisely, as \textit{"a statically typed, data-parallel, and purely functional array language"}, but better than a description, is an example:
\begin{center}
\lstset{language=haskell}
\begin{lstlisting}
let dotprod [n] (xs: [n]f32) (ys: [n]f32): f32 =
	reduce (+) 0f32 (map2 (*) xs ys)

let main [n][m][p] (xss: [n][m]f32) (yss: [m][p]f32): [n][p]f32 =
	map (\xs -> map (dotprod xs) (transpose yss)) xss
\end{lstlisting}%
\captionof{lstlisting}{Matrix-matrix multiplication in Futhark \cite{ppopp}}
\label{matmultFuthark}
\end{center}
A Futhark program for matrix-matrix multiplication can be seen in listing \ref{matmultFuthark}, the syntax is similar to languages such as ML, and Haskell. It is a good example of how Futhark differs from CUDA or OpenCL (we would have liked to include an example of CUDA, but it was to long, so see \ref{cuda-matmult} for that). It allows the programmer to write efficient parallel code, without all the domain specific knowledge regarding massively parallel systems. 

The reason why Futhark is a functional language, is that functional languages lends themselves well to parallel problems, due to the functional paradigm not relying on evaluation order an side effects, as much as other many other paradigms. An example of this is \texttt{map}, which can process each element in parallel. However there are still issues, with the main one being that one parallel thread may not spawn more threads, also called \textit{flat} parallelism. Due to this issue, among others like to small of a stack and no function pointers, a functional language by itself is not a solution, and modifying an existing language such as Haskell is not optimal due to the size and expressiveness of it, being poorly suited for the restrictive nature of a GPU. 

Many problems are not flat. Take the \texttt{dotprod} function from listing \ref{matmultFuthark}. Here there is a \texttt{map2}\footnote{\texttt{map2} is a variant of \texttt{map} taking two arrays instead of a single element and an array. For example \texttt{map2 (+) [1,2,3] [4,5,6] $\to$ [5,7,9]}} which is nested inside of a \texttt{reduce}, both operations are parallel, thereby giving \textit{nested parallelism}. This is the main issue mentioned before, and mapping this nested parallelism into efficient flat parallelism is the most important function performed by the Futhark compiler. 

\subsection{Nested parallelism}
Nested parallelism is when there are parallel constructs nested within each other, a simple example is a perfect loop nest, e.g. nested loops with all the work at the innermost level of the nest.

Futhark supports such nested parallelism. More specifically, Futhark supports nested \textit{data}-parallelism. Data-parallelism is when some function/operation, is applied to a dataset in parallel, such that one thread uses function \texttt{x} on a subset of data \texttt{y}, and another thread also uses function \texttt{x}, but on a different subset of \texttt{y}. Other languages/environments that support nested data-parallelism are the likes of; CUDA, Open MP, OpenCL, NESL etc. With NESL \cite{nesl} being the conceptual precursor to the other languages. NESL solved the issue of nested data-parallelism by flattening data structures (both regular and irregular), by a process called flattening \cite{flat}. NESL solution however is often inefficient, due to it always maximizing available parallelism, which can lead to overhead, that is greater than exploiting less of the parallel construct. It uses a lot of memory, due to copies of the data, and finally, locality-of-reference optimizations are basically impossible due to poor access pattern information \cite{futhark-moderate-blog}.

\subsection{Moderate flattening}
In the world of GPU's we have \textit{kernel}s, a kernel is simply a GPU program. In CUDA specifically, it allows the programmer to define a function, that, when called, is executed \textit{N} times in parallel by \textit{N} different \textit{CUDA threads} or \textit{CUDA blocks}\footnote{For a futher explanation see threads and blocks see \cite{prog-guide-cuda}}. Such a function is called a kernel, and an example of a CUDA kernel is the matrix-matrix multiplication in appendix \ref{cuda-matmult}. 

A kernel can be thought of as the work, that is to be done, in a perfect \textit{parallel nest}. A perfect parallel nest is a nest where all the work inside the nest, is at the innermost level of the nest. 
\begin{figure}
\centering
	\begin{minipage}{0.45\textwidth}
	\centering
	\begin{lstlisting}
map (\xs -> let y = reduce (+) 0 xs 
		in map (+y) xs)
	xss
	\end{lstlisting}
	\captionof{lstlisting}{Code fragment before distribution.} \label{beforeDist}
	\end{minipage}\hspace*{\fill}
	\begin{minipage}{0.45\textwidth}
	\centering
	\begin{lstlisting}
let ys = map (\xs -> reduce (+) 0 xs) xss
in map (\xs y -> map (+y) xs) xss ys
	\end{lstlisting}
	\captionof{lstlisting}{Code fragment after distribution.} \label{afterDist}
	\end{minipage}
	\captionof{figure}{Two version of a fragment of code, showing the distribution of SOACs (\textit{second-order array combinators}) \cite{futhark-moderate-blog}.}
	\label{loopDist}
\end{figure}
\noindent An example of nested data-parallelism and transformation to kernels, can be seen in figure \ref{loopDist}. We can see that the outer \texttt{map}, of listing \ref{beforeDist}, contains more parallelism in it, in terms of the parallel constructs \texttt{reduce} and \texttt{map}, so in the case where the outer \texttt{map} does not saturate the GPU, we would want to exploit the parallelism of the \texttt{map} body. With listing \ref{beforeDist} this is not possible, since the outer \texttt{map} would be parallelised, and the inner sequentialized to one GPU kernel. Instead we can transform it to listing \ref{afterDist}. Here the SOACs of the body (\texttt{map} and \texttt{reduce}) in listing \ref{beforeDist}, are distributed out into their own \texttt{map} nests, giving two perfect \texttt{map} nests, and thereby translates to two GPU kernels, where the first kernel of line 1, is passed into the second kernel of line 2, allowing for further exploitation of the nested parallelism.

This transformation is dubbed \textit{moderate flattening} \cite{futhark-nested-para}, due to its conceptual resemblance to the flattening algorithm put forth by \citeauthor{flat} \cite{flat}. However it is moderate in it approach, due to it flattening the parallel construct until the parallelism saturates the hardware, after which it efficiently sequentialize the remaining parallelism. While flattening supports both regular and irregular structures, Futhark only supports regular structures, and will sequentialize the irregular structures (however Futhark, and its creator documents how to optimize for regular structures). As we saw in Figure \ref{loopDist}, Futhark flattens nested parallel construct by extracting kernels, based on rewrite/flattening rules. At the time of implementing moderate flattening, the algorithm used to apply these rules where based on heuristics about the structure of \texttt{map} nest contexts. These where good approximations but due to the nature of GPU's (having different capacities for parallelism) there is no one size fits all. 


\subsection{Incremental flattening}
Moderate flattening relied on heuristics, to appropriately flatten the parallelism in a Futhark program, however, as mentioned, this is a good approximation but it lacks the ability to dynamically flatten appropriately to the hardware capabilities, and the size of the data being worked on. This is where incremental flattening comes in. In short incremental flattening statically generates multiple different, semantically equivalent, piecewise code versions of the same program, based on the rules of moderate flattening (along with some additional ones) \cite{inc-flat}, and then dynamically chooses whether or not to further exploit parallelism or not.  

To get an intuition for incremental flattening, lets go back to the matrix-matrix multiplication example in listing \ref{matmultFuthark}. Due to matrix-matrix multiplication containing a lot of \texttt{map}s (as do many GPU programs), we will briefly explain the code versions generated by the flattening rule, when a \texttt{map} containing additional parallelism (CV is short for code version); 
\begin{itemize}
\item[CV0] The body of the map will be executed sequentially. This means that there will be assigned one GPU thread to each element of the array being mapped over, each thread executing the map body sequentially.
\item[CV1] The body of the map is partially executed in parallel. This means that there will be assigned one GPU group/block to each element of the array being mapped over. This is partial since it does not exploit the parallelism fully, due to not having enough threads to fully parallelise the entire construct within the \texttt{map} body.
\item[CV2] Continues to flatten the \texttt{map} function, since we have still have further parallel capacity to exploit.  
\end{itemize}
\begin{center}
	\centering 
	\input{maptree}
	\captionof{figure}{A tree showing the structure of the three different code versions generate by flattening a \texttt{map} nest}
	\label{maptree}
\end{center}
In matrix-matrix multiplication, there are three levels of nested parallelism to exploit; the outer and inner \texttt{map} on line 5, and the \texttt{dotprod} function. Lets look at the different ways we can execute the function, there are at least 5 different ways to execute the code, generating 5 different code versions (denoted by a V) \cite{inc-flat};
\begin{itemize}
	\item[V0)] The outer \texttt{map} is distributed out across the parallel construct, executing the body of the function sequentially. More specifically one GPU thread calculates one row in the resulting matrix each. This is CV1 for \texttt{map}.
	\item[V1)] The outer \texttt{map} is executed in parallel, and the inner \texttt{map} of \texttt{main} is executed partially in parallel. More specifically one GPU group/block calculates one row in the resulting matrix. This is CV2 for \texttt{map}. 
	\item[V2)] The two outer maps of \texttt{main}, are extracted as kernels and the kernel of the outer \texttt{map} will invoke the inner \texttt{map} kernel saturating the GPU, and the body of the two \texttt{map}s (\texttt{dotprod}) is executed sequentially. More specifically each GPU thread is calculating one element in the resulting matrix. This is CV1 for \texttt{map}.
	\item[V3)] The two outer \texttt{maps}, does not saturate the GPU, but the \texttt{dotprod} function would oversaturate it, therefore is \texttt{dotprod} partially executed in parallel. More specifically each element of the result matrix is executed by a GPU group/block. This is CV2
	\item[V4)] If the entire parallel nest does not oversaturate the GPU, the entire nest is executed in parallel\footnote{Reduce does not appear to be a parallel construct, due to the elements being dependent on each other, however this is done by a \textit{segmented reduce} \cite{segred}}
\end{itemize} 
\begin{comment}
\begin{inparadesc}
	\item[0)] We don't really know what it would do here.
	\item[1)] The first map of \texttt{main} saturates the GPU, and the \texttt{map} is executed in parallel, with the body sequentialized (this is unlikely in a perfect \texttt{map} nest such as this). %er stadig usikker på denne kode version
	\item[2)] The two outer maps of \texttt{main}, saturates the GPU, and are therefore executed in parallel with \texttt{dotprod} function executed sequentially.
	\item[3)] If the parallelism in the two outer \texttt{maps}, does not saturate the GPU, but the \texttt{reduce-map} nest would oversaturate it, a transformation called \textit{Interchange Reduce With Inner Map (IRWIM)} is performed to interchange the \texttt{reduce-map} nest, thereby executing the resulting three maps in parallel, and executing the reduce sequentially.  
	\item[4)] If the entire parallel nest does to oversaturate the GPU, the entire nest is executed in parallel\footnote{Reduce does not appear to be a parallel construct, due to the elements being dependent on each other, however this is done by a \textit{segmented reduce} \cite{segred}}.
\end{inparadesc} 
\end{comment}
We have to choose between these different, but semantically equivalent, ways of execution. The optimal choice will depend on the hardware, and the data being worked on. To visualize the choices, we will look at them as a tree. In Figure \ref{MatMultTree} there are four choices, we have to consider, and these choices are dependent on each other
\begin{center}
	\centering 
	\input{MatMultTree}
	\captionof{figure}{The structure of choices found in the futhark program for matrix-matrix multiplication (see listing \ref{matmultFuthark}). (V) represents the resulting code version of a choice}
	\label{MatMultTree}
\end{center}
With these dependent choices, the idea of incremental flattening becomes increasingly clear. We step though the code, and decided whether we should flatten the parallelism available (for example a \texttt{map} nest), or whether we should sequentialize it, and exploit locality-of-reference optimizations. Due to the dependency of the choices, we end up iterating through the parallel nest, and flattening until we reach the full capacity of the hardware, hence the name \textit{incremental flattening}. The choices will not always be in form of a unbalanced tree (although they are for the most part), they can also be balanced, and/or form a forest, we will look more closely at this later.
\section{Autotuning}
The process of autotuning a Futhark program is, to automatically pick the fastest combination (or single) code versions, based on the hardware the program is executed on, and some representative data. In the process of selecting the appropriate code version we have three important terms we, that we be central to this report, and therefore we wish to clearly define them in the context of this report;
\begin{basedescript}{
		\desclabelstyle{\multilinelabel}
		\desclabelwidth{2cm}}
	\item [Threshold parameter] or just \textit{threshold} is a value that symbolizes the capacity for parallelism (memory, thread count etc.), of the hardware the program is executed on. 
	\item [Dataset value] is a value that represent some of the dataset given. For example for the outer \texttt{map} in matrix-matrix multiplication, the dataset value would (primarily) be constructed by the number of rows.
	\item [Predicate] threshold comparison, or just comparison, is a comparison between a threshold and a dataset value that guard which code version to use.   
\end{basedescript}
To pick the best version, we need to tune the threshold parameters, to values that will result en the combination of code versions that gives the fastest runtime. This process is called tuning. The guarding predicates are of the form $threshod \; \leq \; dataset\; value$. By default the threshold parameter is set to a value off $2^{15}$, as an estimate, but is likely sub-optimal. With these terms, we can now fill out the example in Figure \ref{MatMultTree}, getting Figure \ref{MatMultTreeFilled}. 
\begin{center}
	\centering 
	\input{MatMultTreeFilled}
	\captionof{figure}{The tree generated by matrix-matrix multiplication that is to be tuned. ($t_i$) is a threshold, ($dVal_i$) is a dataset value, and (T) and (F) are indicative of the path we take, based on the threshold comparison.}
	\label{MatMultTreeFilled}
\end{center}
To inspect the structure of these predicates and thresholds parameters further, lets look at a more complex Futhark program, from Futhark bench, called \textit{LocVolCalib};
\begin{center}
	\centering
	\input{LocValCalibTree}
	\captionof{figure}{The dependencies between thresholds, of the test program \texttt{LocVolCalib.fut}. (Th) is a threshold comparison, (V) is a code version, and (T) and (F) are indicative of the path we take, based on the threshold comparison}
	\label{LocVolCalibTree}
\end{center}
\noindent To tune a program we need to examine each execution path through the tree. It is important not to get an end node (code version) confused with a version of the program. Two example of paths through the tree in Figure \ref{LocVolCalibTree}, that shows this, could be;
\begin{itemize}
	\item \texttt{\{(Th4, False), (Th5, False), (Th6, False), (Th7, True)\}}
	\item \texttt{\{(Th4, False), (Th5, False), (Th6, False), (Th7, False), (Th8, False), (Th9, True), (Th16, False), (Th17, True)\}}
\end{itemize}  
The first path is simple, the code represented by \texttt{T4, T5, T6} is executed in parallel, where everything after it, is executed sequentially. The second path is more interesting, \texttt{T7} has two child nodes, that are reached with a false comparison. Here it is clear that two end nodes are reached, namely \texttt{(V6, V9)}, and these two code versions are then combined into one program. This is also important to note, because we could have a forest, instead of a single tree, and this would leave multiple independent code versions, that are to be combined.

