\section{Background}
\subsection{Futhark}
A common way to increase computer performance, is to increase the capacity for parallelism. For practical usage, however, this is difficult to implement, due to low-level GPU-specific languages requiring domain specific knowledge to make full use of that capacity. A wast amount of work has gone into transforming high-level hardware-agnostic code into these low-level GPU-specific languages \cite{inc-flat}. 

The programming language \textbf{Futhark} aims to solve this problem. The creator of Futhark writes the purpose, of the language, nicely on the home page for the language \textit{"Because itâ€™s nicer than writing CUDA or OpenCL by hand!"} \cite{futhark-home}. On the same page, Futhark is described, more precisely, as \textit{"a statically typed, data-parallel, and purely functional array language"}, but better than a description, is an example:
\begin{center}
\lstset{language=haskell}
\begin{lstlisting}
let dotprod [n] (xs: [n]f32) (ys: [n]f32): f32 =
	reduce (+) 0f32 (map2 (*) xs ys)

let main [n][m][p] (xss: [n][m]f32) (yss: [m][p]f32): [n][p]f32 =
	map (\xs -> map (dotprod xs) (transpose yss)) xss
\end{lstlisting}%
\captionof{lstlisting}{Matrix-matrix multiplication in Futhark \cite{ppopp}}
\label{matmultFuthark}
\end{center}
A Futhark program for matrix-matrix multiplication can be seen in listing \ref{matmultFuthark}, the syntax is similar to languages such as ML, and Haskell. It is a good example of how Futhark differs from CUDA or OpenCL (we would have liked to include an example of CUDA, but it was to long, so see \ref{cuda-matmult} for that). It allows the programmer to write efficient parallel code, without all the domain specific knowledge regarding massively parallel systems. 
%Mentioned that the compiler optimizses with the static funct etc.

\subsection{Flattening}
\label{flattening}
%It is difficult to exploit nested data-parallelism. A common approach is to add data-parallel operations, such as \texttt{map}, to existing languages, for example High Performance Fortran. However, this approach is not well suited for irregular data structures, such as trees or graphs \cite{nesl}.
It is difficult to exploit nested data-parallelism. An approach to this problem is flattening \cite{flat}. The aim is to transform nested parallelism, into one-level parallelism. To briefly understand the concept of
% More general example for A
flattening, we will flatten a nested collection; To flatten a collection, we represent it as a \texttt{field} \cite{flat}, here we will represented a \texttt{field} as a tuple, but it could be a different data-structure. The second element of the \texttt{field} tuple, is either a collection, or another \texttt{field}, the first element, is a collection representing the shape of the second element; \texttt{field = (shape, val)}. So for a flat collection (\texttt{A}) the field wold be, \texttt{([length(a)], A)}, where brackets represent a collection, and the shape being the length, due to it being flat.
%let \texttt{A = [1, 2, 3, 4, 5, 6]}, this can be flattened into what is called a \textbf{field}   The collection \texttt{A}, flattened into a field, would be \texttt{(Ashape = [6], Aval = [1, 2, 3, 4, 5, 6])}. 
The \texttt{field} structure can be applied recursively to any depth. A depth of \texttt{n} would give \texttt{n} shape collections, for example let \texttt{B = [[1, 2], [3, 4, 5], [6]]}, this would be flattened to the field \texttt{B = ([3], (Bshape, Bval)) $\to$ B = ([3], ([2, 3, 1], [1, 2, 3, 4, 5, 6]))}. 

With the collection flattened to a field structure, work can now be applied in parallel, by stepping up or down in the field, for example taking the head of each nested collection would step down in \texttt{B} once, to get \texttt{step-down([3], ([3], ([2, 3, 1], [1, 2, 3, 4, 5, 6]))) = ([2, 3, 1], [1, 2, 3, 4, 5, 6])}, and then distributing the head function to each segment in parallel, by giving each segment of \texttt{[1..6]} to a core and performing the function, giving back the field \texttt{([3], [1, 3, 6])}.\\\\
%We could possibly expand on this
With the ability to flatten, nested parallelism can be mapped to hardware, as one-level. However this is not necessarily optimal, as it does not take the capability of the hardware into account, leading to poor utilization of locality optimization. The problem then becomes how to optimize for nested parallelism, hardware capabilities, and the data being worked on. This is what the following section of \textit{incremental flattening} addresses.

\subsection{Incremental flattening}
To illustrate the need for incremental flattening, lets go back to the matrix-matrix multiplication example in listing \ref{matmultFuthark}. There are 3 levels of nested parallelism to exploit the outer and inner \texttt{map} on line 5, and the \texttt{dotprod} function. Instead of trying to completely flatten all three levels, lets look at the options we have. With these three levels there is, at least, 3(4) different ways to execute the code \cite{inc-flat};
\begin{inparadesc}
	\item[1)] The entire function should be run sequentially.
	\item[2)] Otherwise \texttt{dotprod} should be sequentialized, to optimize locality.
	\item[3)] If the parallelism in the two \texttt{maps}, would over-saturate the hardware, then another chunk can be sequentialized, to optimize locality. 
	\item[4)] If the size of the two maps is not big enough to saturate the hardware, then the \texttt{dotprod} function should also be executed in parallel.
\end{inparadesc} 
The optimal choice here will, as stated in the end of section \ref{flattening}, depend on the hardware, and the data being worked on. To visualize the choices further, lets look at them as a tree. In tree \ref{MatMultTree} there are 4 choices, as mentioned prior, and these choices are dependent on each other; \begin{inparadesc}
	\item[V1)] Is the case where the entire matrix-matrix multiplication is executes sequentially.
	\item[V2)] Is the case where the outer \texttt{map} (of line 5 in listing \ref{matmultFuthark}) is execute in parallel, and the rest sequentially. 
	\item[V3)] Is where both the inner and outer \texttt{map} is execute in parallel, and the \texttt{dotprod} function (of line 1) is executed sequentially.  
	\item[V4)] is the case where everything is executed in parallel.
\end{inparadesc} 
\begin{center}
	\centering 
	\input{MatMultTree}
	\captionof{figure}{The structure of choices found in the futhark program for matrix-matrix multiplication (see listing \ref{matmultFuthark}). (T) and (F), symbolises the result of the choice, and which path to take based on it, and (V) represents the outcome of the choice}
	\label{MatMultTree}
\end{center}
Futhark has taken an approach to this choice, mentioned in the section above, where it generates several piecewise, semantically equivalent, code versions of the same code in a program, each exploiting a different level of parallelism, which are then discriminated at runtime by statically generated predicates \cite{inc-flat}.

The process of picking a code version is done using the generated predicates, and \textit{threshold parameters}, and is called tuning, we tune the parameters to get the best runtime based on the hardware, and data given. Currently the predicates are a simple less-than comparison, but that could change. By default the threshold parameter is set to a value off $2^{15}$, as an estimate, but is likely sub-optimal. Due to the way the code versions are generated, they can be dependent on each other. If we think back to the example in listing \ref{matmultFuthark}, we saw that there were, at least, 3 different executions regarding exploitation of parallelism, this means that there are also 3 code versions, guarded by predicates. If the predicate for the two outer \texttt{maps} shows they would saturate the GPU, then the code version where the parallelism would over-saturate the GPU, and run the entire function sequentially, would not get picked, we then step down into the \texttt{dotprod} function, here there is a predicate that decides if \texttt{dotprod} should also be executed in parallel, or whether it would over-saturate, and should be executed sequentially. From this it is clear that the predicates/threshold are dependent on each other.

To inspect the structure of these predicates and thresholds parameters further, lets look at an actual Futhark program;
\begin{center}
	\centering
	\input{LocValCalibTree}
	\captionof{figure}{The dependencies between thresholds, of the test program \texttt{LocVolCalib.fut}. The dependency is based on a comparison of the threshold (Th), the edge to the next node is then taken based on that comparison, with either a true (T), or false (F) value. Each path through the tree, to one, or more, code versions (V), is then an execution of the program.}
	\label{LocVolCalibTree}
\end{center}
\noindent To tune a program we need to examine each execution path through the tree. It is important not to get an end node confused with an execution path. Two example of paths through the tree in figure \ref{LocVolCalibTree}, that show this, could be;
\begin{itemize}
	\item \texttt{\{(T4, False), (T5, False), (T6, False), (T7, True)\}}
	\item \texttt{\{(T4, False), (T5, False), (T6, False), (T7, False), (T8, False), (T9, True), (T16, False), (T17, True)\}}
\end{itemize}  
The first path is simple, the code represented by \texttt{T4, T5, T6} is executed in parallel, where everything after it, is executed sequentially. The second path is more interesting, \texttt{T7} has two child nodes, that are reached with a false comparison. Here it is clear that two end nodes are reached, namely \texttt{(V6, V9)}, and these two code versions are then combined into one program. This is also important to note, because we could have a forest, instead of a single tree, and this would leave multiple independent code versions, that are to be combined.


\begin{comment}
\subsection{Threshold parameters}
To choose the proper code version for the program, based on representative data, we need something to base our choices from. At runtime, Futhark generates some parameters we can use as this basis. These parameters represent a piece of codefi

\subsection{Structure of the threshold parameter}
% Vi har nok skrevet det med dependencies fÃ¸r, men sÃ¥ kan vi bare lige rykke det/flette sammen
The aforementioned threshold parameters, can be dependent on each other. Imagine a nested loop where it is determined, for the outer loop, that it should not be executed in parallel, therefore the inner loop should also not be executed in parallel, thereby making the inner dependent on the outer. These dependencies builds a tree, which an example of can be seen in figure \ref{LocVolCalibTree}.
\begin{figure}[H]
\centering
\input{Tree}
\caption{The dependencies between thresholds, of the test program \texttt{LocVolCalib.fut}. The dependency is based on a comparison of the threshold (T), the edge to the next node is then taken based on that comparison. Each path through the tree, to one, or more, end nodes, is then an execution of the program.}
\label{LocVolCalibTree}
\end{figure}
\noindent To tune a program we need to examine each execution path. It is important not to get an end node confused with an execution path. Two example of paths through the tree in figure \ref{LocVolCalibTree}, that show this, could be;
\begin{itemize}
\item \texttt{\{(T4, False), (T5, False), (T6, False), (T7, True)\}}
\item \texttt{\{(T4, False), (T5, False), (T6, False), (T7, False), (T8, False), (T9, True), (T16, False), (T17, True)\}}
\end{itemize} 
The first path is simple, the code represented by \texttt{T4, T5, T6} is executed in parallel, where everything after it, is executed sequentially. The second path is more interesting, \texttt{T7} has two child nodes, that are reached with a false comparison. Here it is clear that two end nodes is reached, namely \texttt{(E6, E9)}. This is also important to note, because we could have a forest, instead of a single tree, and this would leave multiple end nodes.
\end{comment}

